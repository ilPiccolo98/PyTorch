{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPhcOmZNVqT3XUYK5AdXh7A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FCcugv6lRQkJ","executionInfo":{"status":"ok","timestamp":1723639449997,"user_tz":-120,"elapsed":5,"user":{"displayName":"Giuseppe Piccolo","userId":"17844279195123924050"}},"outputId":"5c57b0db-5370-46d9-e7b5-8ab1a56ef931"},"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction before training: f(5) = 0.000\n","epoch 1: weight = 1.200, loss = 30.0\n","epoch 2: weight = 1.680, loss = 4.7999992\n","epoch 3: weight = 1.872, loss = 0.76800019\n","epoch 4: weight = 1.949, loss = 0.12288\n","epoch 5: weight = 1.980, loss = 0.019660834\n","epoch 6: weight = 1.992, loss = 0.0031457357\n","epoch 7: weight = 1.997, loss = 0.00050330802\n","epoch 8: weight = 1.999, loss = 8.0531863e-05\n","epoch 9: weight = 1.999, loss = 1.2884395e-05\n","epoch 10: weight = 2.000, loss = 2.0613531e-06\n","epoch 11: weight = 2.000, loss = 3.2973401e-07\n","epoch 12: weight = 2.000, loss = 5.2823452e-08\n","epoch 13: weight = 2.000, loss = 8.4878167e-09\n","epoch 14: weight = 2.000, loss = 1.3369572e-09\n","epoch 15: weight = 2.000, loss = 2.1679014e-10\n","epoch 16: weight = 2.000, loss = 3.5313974e-11\n","epoch 17: weight = 2.000, loss = 5.0768278e-12\n","epoch 18: weight = 2.000, loss = 8.9883656e-13\n","epoch 19: weight = 2.000, loss = 1.3145041e-13\n","epoch 20: weight = 2.000, loss = 1.3145041e-13\n","Prediction after training: f(5) = 10.000\n"]}],"source":["import numpy as np\n","\n","#f = w * x\n","#f = 2 * x\n","\n","x = np.array([1, 2, 3, 4], dtype=np.float32)\n","y = np.array([2, 4, 6, 8], dtype=np.float32)\n","\n","w = 0.0\n","\n","#model prediction\n","def forward(x):\n","  return w * x\n","\n","#loss = MSE\n","def loss(y, y_predicted):\n","  return ((y_predicted - y) ** 2).mean()\n","\n","#gradient\n","#MSE = 1/N * (w * x - y) ** 2\n","#dJ/dw = 1/N 2x (w * x - y)\n","def gradient(x, y, y_predicted):\n","  return np.dot(2 * x, y_predicted - y).mean()\n","\n","print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n","\n","#training\n","learning_rate = 0.01\n","n_iters = 20\n","\n","for epoch in range(n_iters):\n","  #prediction = forward pass\n","  y_pred = forward(x)\n","  #loss\n","  l = loss(y, y_pred)\n","  #gradients\n","  dw = gradient(x, y, y_pred)\n","  #update weights\n","  w -= learning_rate * dw\n","  if epoch % 1 == 0:\n","    print(f\"epoch {epoch + 1}: weight = {w:.3f}, loss = {l:.8}\")\n","\n","print(f\"Prediction after training: f(5) = {forward(5):.3f}\")"]},{"cell_type":"code","source":["# 1) Design model (input, output size, forward pass)\n","# 2) Construct loss and optimizer\n","# 3) training loop\n","# - forward pass: compute prediction\n","# - backword pass: gradients\n","# - update weights\n","\n","import torch\n","import torch.nn as nn\n","\n","#f = w * x\n","#f = 2 * x\n","\n","x = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n","y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n","x_test = torch.tensor([5], dtype=torch.float32)\n","\n","n_samples, n_features = x.shape\n","\n","input_size = n_features\n","output_size = n_features\n","#model = nn.Linear(input_size, output_size)\n","\n","class LinearRegression(nn.Module):\n","  def __init__(self, input_dim, output_dim):\n","    super(LinearRegression, self).__init__()\n","    self.lin = nn.Linear(input_dim, output_dim)\n","\n","  def forward(self, x):\n","    return self.lin(x)\n","\n","model = LinearRegression(input_size, output_size)\n","\n","print(f\"Prediction before training: f(5) = {model(x_test).item():.3f}\")\n","\n","#training\n","learning_rate = 0.01\n","n_iters = 100\n","\n","loss = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(n_iters):\n","  #prediction = forward pass\n","  y_pred = model(x)\n","  #loss\n","  l = loss(y, y_pred)\n","  #gradients = backward pass\n","  l.backward() #dl/dw\n","  #update weights\n","  optimizer.step()\n","  #zero gradients\n","  optimizer.zero_grad()\n","\n","  if epoch % 10 == 0:\n","    [w, b] = model.parameters()\n","    print(f\"epoch {epoch + 1}: weight = {w[0][0].item():.3f}, loss = {l:.8}\")\n","\n","print(f\"Prediction after training: f(5) = {model(x_test).item():.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KFIZZ8H3UgFO","executionInfo":{"status":"ok","timestamp":1723640226429,"user_tz":-120,"elapsed":252,"user":{"displayName":"Giuseppe Piccolo","userId":"17844279195123924050"}},"outputId":"3c425606-5d80-4418-d12f-a5dcb2a07606"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction before training: f(5) = -0.862\n","epoch 1: weight = 0.049, loss = 33.404358\n","epoch 11: weight = 1.380, loss = 1.0546079\n","epoch 21: weight = 1.603, loss = 0.20656309\n","epoch 31: weight = 1.648, loss = 0.17418718\n","epoch 41: weight = 1.663, loss = 0.16352199\n","epoch 51: weight = 1.674, loss = 0.15399057\n","epoch 61: weight = 1.684, loss = 0.14502716\n","epoch 71: weight = 1.693, loss = 0.13658577\n","epoch 81: weight = 1.702, loss = 0.12863585\n","epoch 91: weight = 1.711, loss = 0.12114856\n","Prediction after training: f(5) = 9.421\n"]}]}]}